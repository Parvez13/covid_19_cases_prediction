{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a46ecf2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <h1 align=\"center\">Covid-19 Cases</h1>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d51773",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "**Coronavirus disease 2019 (COVID-19)** is a contagious disease caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The first known case was identified in *Wuhan, China*, in December 2019. The disease has since spread worldwide, leading to an ongoing pandemic.\n",
    "\n",
    "### Symptoms\n",
    "Symptoms of COVID-19 are variable, but often include *fever, cough, headache, fatigue, breathing difficulties, and loss of smell and taste*.Symptoms may begin *one to fourteen days* after exposure to the virus. At least a third of people who are infected do not develop noticeable symptoms. Of those people who develop symptoms noticeable enough to be classed as patients, most (81%) develop mild to moderate symptoms (up to mild pneumonia), while 14% develop severe symptoms (dyspnea, hypoxia, or more than 50% lung involvement on imaging), and 5% suffer critical symptoms (respiratory failure, shock, or multiorgan dysfunction). Older people are at a higher risk of developing severe symptoms. Some people continue to experience a range of effects (long COVID) for months after recovery, and damage to organs has been observed. Multi-year studies are underway to further investigate the long-term effects of the disease.\n",
    "\n",
    "### Transmutation\n",
    "COVID-19 transmits when people breathe in air contaminated by droplets and small airborne particles containing the virus. The risk of breathing these in is highest when people are in close proximity, but they can be inhaled over longer distances, particularly indoors. Transmission can also occur if splashed or sprayed with contaminated fluids in the eyes, nose or mouth, and, rarely, via contaminated surfaces. People remain contagious for up to 20 days, and can spread the virus even if they do not develop symptoms.\n",
    "\n",
    "Several testing methods have been developed to diagnose the disease. The standard diagnostic method is by detection of the virus' nucleic acid by real-time reverse transcription polymerase chain reaction (rRT-PCR), transcription-mediated amplification (TMA), or by reverse transcription loop-mediated isothermal amplification (RT-LAMP) from a nasopharyngeal swab.\n",
    "\n",
    "Several COVID-19 vaccines have been approved and distributed in various countries, which have initiated mass vaccination campaigns. Other preventive measures include physical or social distancing, quarantining, ventilation of indoor spaces, covering coughs and sneezes, hand washing, and keeping unwashed hands away from the face. The use of face masks or coverings has been recommended in public settings to minimize the risk of transmissions. While work is underway to develop drugs that inhibit the virus, the primary treatment is symptomatic. Management involves the treatment of symptoms, supportive care, isolation, and experimental measures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b958cb",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc0b86c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting comet_ml\n",
      "  Using cached comet_ml-3.19.0-py2.py3-none-any.whl (299 kB)\n",
      "Collecting requests-toolbelt>=0.8.0\n",
      "  Using cached requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "Collecting websocket-client>=0.55.0\n",
      "  Using cached websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n",
      "Collecting dulwich>=0.20.6\n",
      "  Using cached dulwich-0.20.25-cp38-cp38-win_amd64.whl (488 kB)\n",
      "Requirement already satisfied: six in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from comet_ml) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from comet_ml) (1.12.1)\n",
      "Collecting everett[ini]>=1.0.1\n",
      "  Using cached everett-2.0.1-py2.py3-none-any.whl (33 kB)\n",
      "Collecting wurlitzer>=1.0.2\n",
      "  Using cached wurlitzer-3.0.2-py3-none-any.whl (7.3 kB)\n",
      "Collecting semantic-version>=2.8.0\n",
      "  Using cached semantic_version-2.8.5-py2.py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: requests>=2.18.4 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from comet_ml) (2.26.0)\n",
      "Collecting nvidia-ml-py3>=7.352.0\n",
      "  Using cached nvidia_ml_py3-7.352.0-py3-none-any.whl\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from comet_ml) (3.2.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from dulwich>=0.20.6->comet_ml) (2021.10.8)\n",
      "Requirement already satisfied: urllib3>=1.24.1 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from dulwich>=0.20.6->comet_ml) (1.26.7)\n",
      "Collecting configobj\n",
      "  Using cached configobj-5.0.6-py3-none-any.whl\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.17.3)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (21.2.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (58.0.4)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from requests>=2.18.4->comet_ml) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\prvzs\\onedrive\\desktop\\desktop\\datasets_and_projects\\covid_19_cases\\env\\lib\\site-packages (from requests>=2.18.4->comet_ml) (3.2)\n",
      "Installing collected packages: everett, configobj, wurlitzer, websocket-client, semantic-version, requests-toolbelt, nvidia-ml-py3, dulwich, comet-ml\n",
      "Successfully installed comet-ml-3.19.0 configobj-5.0.6 dulwich-0.20.25 everett-2.0.1 nvidia-ml-py3-7.352.0 requests-toolbelt-0.9.1 semantic-version-2.8.5 websocket-client-1.2.1 wurlitzer-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ea6b580",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/parvezsohail/covid-19-cases/439c5da3c78b4caebf01c8cde54b795f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # import comet_ml at the top\n",
    "from comet_ml import Experiment\n",
    "\n",
    "# Create an experiment with api key\n",
    "experiment = Experiment(\n",
    "    api_key=\"Z0oOb8S6C70IJ7b2FUcs31MnP\",\n",
    "    project_name='covid_19_cases',\n",
    "    workspace='parvezsohail'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b982e4a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly.express'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17572/1018481721.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m  \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly.express'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import seaborn as  sns\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73f8fe3",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a7834",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df0 = pd.read_csv(\"Dataset/data/CONVENIENT_global_confirmed_cases.csv\")\n",
    "df1 = pd.read_csv(\"Dataset/data/CONVENIENT_global_deaths.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a79f10",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6508a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = df0.iloc[:,1:].columns\n",
    "countries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab66b0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = pd.DataFrame({\"Country\":[],\"Cases\":[]})\n",
    "world['Country'] = df0.iloc[:,1:].columns\n",
    "cases = []\n",
    "for i in world['Country']:\n",
    "    cases.append(pd.to_numeric(df0[i][1:]).sum())\n",
    "world['Cases'] = cases\n",
    "\n",
    "country_list = list(world['Country'].values)\n",
    "idx = 0\n",
    "for i in country_list:\n",
    "    sayac = 0\n",
    "    for j in i:\n",
    "        if j==\".\":\n",
    "            i = i[:sayac]\n",
    "            country_list[idx]=i\n",
    "        elif j==\"(\":\n",
    "            i = i[:sayac-1]\n",
    "            country_list[idx]=i\n",
    "        else:\n",
    "            sayac+=1\n",
    "    idx += 1\n",
    "world['Country'] = country_list\n",
    "world = world.groupby('Country')['Cases'].sum().reset_index()\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "continent = pd.read_csv(\"Dataset/continents/continents2.csv\")\n",
    "continent[\"name\"] = continent[\"name\"].str.upper()\n",
    "continent.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfeb7443",
   "metadata": {},
   "source": [
    "## Data Viualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bea8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "world['Cases Range'] = pd.cut(world['Cases'],[10000,50000,200000,800000,1500000,15000000],labels=[\"U50K\",\"50kto200k\",\"200kto800k\",\"800kto1.5M\",\"1.5M+\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddbfcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = []\n",
    "for i in world['Country'].str.upper().values:\n",
    "    if i == \"BRUNEI\":\n",
    "        i = \"BRUNEI DARUSSALAM\"\n",
    "    elif i == \"US\":\n",
    "        i = \"UNITED STATES\"\n",
    "    if len(continent[continent[\"name\"] == i][\"alpha-3\"].values)==0:\n",
    "        alpha.append(np.nan)\n",
    "    else:\n",
    "        alpha.append(continent[continent[\"name\"]==i][\"alpha-3\"].values[0])\n",
    "world[\"Alpha3\"]=alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4360ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5789e10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "world['Country'] = world['Country'].str.upper()\n",
    "world.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0419d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "world.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6db302",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.choropleth(world.dropna(),\n",
    "                   locations='Alpha3',\n",
    "                   color='Cases Range',\n",
    "                   projection='mercator',\n",
    "                   color_discrete_sequence=['khaki','yellow','lightblue','red','orange'])\n",
    "fig.update_geos(fitbounds='locations',visible=False)\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3dd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily cases all around the world\n",
    "count = []\n",
    "for i in range(1,len(df0)):\n",
    "    count.append(sum(pd.to_numeric(df0.iloc[i,1:].values)))\n",
    "\n",
    "df = pd.DataFrame()\n",
    "df['Date'] = df0['Country/Region'][1:]\n",
    "df['Cases'] = count\n",
    "df = df.set_index('Date')\n",
    "\n",
    "# Daily death cases all around the world\n",
    "count = []\n",
    "for i in range(1,len(df1)):\n",
    "    count.append(sum(pd.to_numeric(df1.iloc[i,1:].values)))\n",
    "\n",
    "df['Deaths'] = count\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9abd86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove decimal values\n",
    "pd.set_option('precision',0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d315504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily covid19 cases\n",
    "\n",
    "plt.ticklabel_format(style='plain')\n",
    "df.Cases.plot(title='Daily Covid19 Cases in World',marker=\".\",figsize=(10,8),label=\"Daily cases\")\n",
    "df.Cases.rolling(window=5).mean().plot(figsize=(25,5),label='MovingAverage(5)')\n",
    "plt.ylabel(\"Cases\",fontsize=15)\n",
    "plt.xlabel(\"Date\",fontsize=15)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f248a79",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = px.line(df, y='Cases',title='Daily Covid 19 Cases in World')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04a578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily covid19 Death Cases\n",
    "df.Deaths.plot(title='Daily Covid19 Deaths in World', marker=\".\",label=\"Daily Deaths\")\n",
    "df.Deaths.rolling(window=5).mean().plot(figsize=(25,5),label='MovingAverage(5)')\n",
    "plt.ylabel(\"Deaths\",fontsize=15)\n",
    "plt.xlabel(\"Date\",fontsize=15)\n",
    "plt.xticks(fontstyle='oblique',fontsize=10)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedf9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df, y='Deaths',title='Daily Covid 19 Death Cases in World')\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8e5d21",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# parse dates from 'df' dataframe\n",
    "set_date = pd.to_datetime(df.index)\n",
    "df.index = set_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f28e1ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f87f80a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get  data array\n",
    "timesteps = df.index.to_numpy()\n",
    "cases = df['Cases'].to_numpy()\n",
    "deaths = df['Deaths'].to_numpy()\n",
    "\n",
    "timesteps[:10],cases[:10],deaths[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cd2e04",
   "metadata": {},
   "source": [
    "## Split dataset into Train and Test\n",
    "\n",
    "The best way to split the time series data is to avoid the random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc0786",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create train and test splits the right way for time series\n",
    "split_size = int(0.8 * len(df))\n",
    "\n",
    "# Create train data splits (everything before the split)\n",
    "X_train, y_train = timesteps[:split_size], cases[:split_size]\n",
    "\n",
    "# Create test data splits (everything after the split)\n",
    "X_test, y_test = timesteps[split_size:], cases[split_size:]\n",
    "\n",
    "len(X_train),len(X_test), len(y_train), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b8db0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot correctly made splits\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.ticklabel_format(style='plain')\n",
    "plt.scatter(X_train, y_train, s=5, label=\"Train data\")\n",
    "plt.scatter(X_test, y_test, s=5, label=\"Test data\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Cases\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720c2022",
   "metadata": {},
   "source": [
    "## Baseline Model : Naive Forecast\n",
    "\n",
    "As usual, let's start with a baseline\n",
    "\n",
    "One of the most common baseline models for time series forecasting, the naive model (also called the`naive forecast`), requires no training at all.\n",
    "\n",
    "That's because all the naive model does is use the previous timestep value to predict the next timestep value\n",
    "\n",
    "The formula looks like this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f938d9e1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a naive forecast\n",
    "naive_forecast = y_test[:-1] # Naive forecast every value excluding the last value\n",
    "naive_forecast[:10], naive_forecast[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c785e456",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Create a function to plot time series data\n",
    "def plot_time_series(timesteps, values, format='.', start=0, end=None, label=None):\n",
    "    \"\"\"\n",
    "    Plots a timesteps (a series of points in time) against values (a series of values across timesteps).\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    timesteps : array of timesteps\n",
    "    values : array of values across time\n",
    "    format : style of plot, default \".\"\n",
    "    start : where to start the plot (setting a value will index from start of timesteps & values)\n",
    "    end : where to end the plot (setting a value will index from end of timesteps & values)\n",
    "    label : label to show on plot of values\n",
    "    \"\"\"\n",
    "    # Plot the series\n",
    "\n",
    "    plt.plot(timesteps[start:end], values[start:end], format, label=label)\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Cases\")\n",
    "    if label:\n",
    "        plt.legend(fontsize=14) # make label bigger\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d013b0f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Plot naive forecast\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.ticklabel_format(style='plain')\n",
    "plot_time_series(timesteps=X_train, values=y_train, label=\"Train data\")\n",
    "plot_time_series(timesteps=X_test, values=y_test, label=\"Test data\")\n",
    "plot_time_series(timesteps=X_test[1:], values=naive_forecast, format=\"-\", label=\"Naive forecast\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d733ccf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4661a07d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# MASE implemented\n",
    "def mean_absolute_scaled_error(y_true, y_pred):\n",
    "    mae = tf.reduce_mean(tf.abs(y_true - y_pred))\n",
    "\n",
    "    mae_naive_no_season = tf.reduce_mean(tf.abs(y_true[1:] - y_true[:-1]))\n",
    "\n",
    "    return mae/mae_naive_no_season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6670ec92",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# evaluate metrics function\n",
    "def evaluate_preds(y_true, y_pred):\n",
    "    # Make sure float32 (for metric calculations)\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float64)\n",
    "\n",
    "    # Calculate various metrics\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true, y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true, y_pred) # puts and emphasis on outliers (all errors get squared)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true, y_pred)\n",
    "\n",
    "    return {\"mae\": mae.numpy(),\n",
    "            \"mse\": mse.numpy(),\n",
    "            \"rmse\": rmse.numpy(),\n",
    "            \"mape\": mape.numpy(),\n",
    "            \"mase\": mase.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa952c9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "naive_results = evaluate_preds(y_true=y_test[1:],\n",
    "                               y_pred=naive_forecast)\n",
    "naive_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d3489d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "### Format Data : Windowing Dataset\n",
    "\n",
    "Windowing is a method to turn a time series dataset into **supervised learning problem**\n",
    "\n",
    "In other words, we want to use windows of the past to predict the future\n",
    "\n",
    "```\n",
    "Window for one month (univariate time series)\n",
    "\n",
    "[0, 1, 2, 3, 4, 5, 6] -> [7]\n",
    "[1, 2, 3, 4, 5, 6, 7] -> [8]\n",
    "[2, 3, 4, 5, 6, 7, 8] -> [9]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7befa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "HORIZON = 7\n",
    "WINDOW_SIZE = 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c562426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to label windowed data\n",
    "def get_labelled_windows(x, horizon=7):\n",
    "    \n",
    "    return x[:,:-horizon], x[:,-horizon:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc360af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the window labelling function\n",
    "test_window, test_label = get_labelled_windows(tf.expand_dims(tf.range(30)+1,axis=0),horizon=HORIZON)\n",
    "print(f\"Window: {tf.squeeze(test_window).numpy()} -> Label: {tf.squeeze(test_label).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf91160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to view NumPy arrays as windows\n",
    "def make_windows(x, window_size=30,horizon=7):\n",
    "    \"\"\"\n",
    "    Turns a 1D array into a 2D array of sequential windows of window size\n",
    "    \"\"\"\n",
    "    # Create a window of specific window_size(add the horizon on the end for later labelling)\n",
    "    window_step = np.expand_dims(np.arange(window_size+horizon),axis=0)\n",
    "    \n",
    "    # Create 2D array of multiple window steps (minus 1 to account for 0 indexing)\n",
    "    window_indexes = window_step + np.expand_dims(np.arange(len(x)-(window_size+horizon-1)),axis=0).T\n",
    "    \n",
    "    # Index on the target array(time series) with 2D array of multiple window steps\n",
    "    #\n",
    "    windowed_array = x[window_indexes]\n",
    "    \n",
    "    # Get the labelled windows\n",
    "    windows, labels = get_labelled_windows(windowed_array, horizon=horizon)\n",
    "    \n",
    "    return windows, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0f407b",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_windows, full_labels = make_windows(cases, window_size=WINDOW_SIZE, horizon=HORIZON)\n",
    "len(full_windows), len(full_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c0c48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 3 windows/labels\n",
    "pd.set_option('precision',0)\n",
    "for i in range(3):\n",
    "    print(f\"Window: {full_windows[i]} -> Label: {full_labels[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568798f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train/test splits\n",
    "def make_train_test_splits(windows, labels, test_split=0.2):\n",
    "    \n",
    "    \"\"\"\n",
    "    Splits matching pairs of windows and labels into train and test splits\n",
    "    \"\"\"\n",
    "    split_size=int(len(windows)*(1-test_split))\n",
    "    train_windows = windows[:split_size]\n",
    "    train_labels = labels[:split_size]\n",
    "    test_windows = windows[split_size:]\n",
    "    test_labels = labels[split_size:]\n",
    "    return train_windows, test_windows, train_labels,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76e52b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_windows, test_windows, train_labels, test_labels = make_train_test_splits(full_windows, full_labels)\n",
    "len(train_windows), len(test_windows), len(train_labels), len(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387267e",
   "metadata": {},
   "source": [
    "## Make a modelling checkpoint\n",
    "\n",
    "In order for a fair comparison, we want to compare each model's best performance against each model's best performance against each model's best performance.\n",
    "\n",
    "For example, if `model_1` performed incredibly well on epoch 55 but its performance fell off toward epoch 100, we want the version of the model's from epoch 55 to compare to other model's rahter than the version of the model from epoch 100.\n",
    "\n",
    "And the same goes for each of our other models:compare the best agoinst the best.\n",
    "\n",
    "To take of this, we'll implement a `ModelCheckpoint` callback.\n",
    "\n",
    "The `ModelCheckpoint callback` will monitor our model's performance during training and save the best model to file by setting `save_best_only=True`.\n",
    "\n",
    "That way when evaluating our model we could restore its best performing configuration from file.\n",
    "\n",
    "🔑 **Note:** Because of the size of the dataset (smaller than usual), you'll notice our modelling experiment results fluctuate quite a bit during training (hence the implementation of the **ModelCheckpoint** callback to save the best model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec52a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a function to implement a ModelCheckpoint callback with a specific filename\n",
    "def create_model_checkpoint(model_name, save_path='model_checkpoint'):\n",
    "    return tf.keras.callbacks.ModelCheckpoint(filepath=os.path.join(save_path, model_name),\n",
    "                                             verbose=0,\n",
    "                                             save_best_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46356edb",
   "metadata": {},
   "source": [
    "## Model 1: Dense Model(Window=30, horizon =7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdd3ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "\n",
    "# set random seed for as reproducile results as possible\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Model\n",
    "model_1 = Sequential(name='model_1_dense')\n",
    "model_1.add(layers.Dense(128, activation='relu'))\n",
    "model_1.add(layers.Dense(HORIZON,activation='linear'))\n",
    "\n",
    "\n",
    "# compile model\n",
    "model_1.compile(loss='mae',\n",
    "               optimizer=tf.keras.optimizers.Adam(),\n",
    "               metrics=['mae'])\n",
    "\n",
    "# Fit the model\n",
    "model_1.fit(x=train_windows,\n",
    "           y=train_labels,\n",
    "           epochs=100,\n",
    "           verbose=1,\n",
    "           batch_size=128,\n",
    "           validation_data=(test_windows,test_labels),\n",
    "           callbacks=[create_model_checkpoint(model_name=model_1.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea967eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate modelon test data\n",
    "model_1.evaluate(test_windows, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95a79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make preds\n",
    "def make_preds(model,input_data):\n",
    "    \n",
    "    forecast = model.predict(input_data)\n",
    "    return tf.squeeze(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596019f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction using model_1 on the test dataset\n",
    "model_1_preds = make_preds(model_1,test_windows)\n",
    "len(model_1_preds), model_1_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71efdb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate preds\n",
    "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels), # reduce to right shape\n",
    "                                 y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ae630",
   "metadata": {},
   "source": [
    "## Make our evaluation function work for larger horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da320310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_preds(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true,dtype=tf.float32)\n",
    "    y_pred = tf.cast(y_pred,dtype=tf.float32)\n",
    "    \n",
    "   # calculate various metric\n",
    "    mae = tf.keras.metrics.mean_absolute_error(y_true,y_pred)\n",
    "    mse = tf.keras.metrics.mean_squared_error(y_true,y_pred)\n",
    "    rmse = tf.sqrt(mse)\n",
    "    mape = tf.keras.metrics.mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mase = mean_absolute_scaled_error(y_true,y_pred)\n",
    "    \n",
    "    # Account for different sized metrics (for longer horizons, reduce to single number)\n",
    "    if mae.ndim > 0:\n",
    "        mae = tf.reduce_mean(mae)\n",
    "        mse = tf.reduce_mean(mse)\n",
    "        rmse = tf.reduce_mean(rmse)\n",
    "        mape = tf.reduce_mean(mape)\n",
    "        mase = tf.reduce_mean(mase)\n",
    "    return {'mae':mae.numpy(),\n",
    "           'mse':mse.numpy(),\n",
    "           'rmse':rmse.numpy(),\n",
    "           'mape':mape.numpy(),\n",
    "           'mase':mase.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53869a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model_3 results aggregated to single values\n",
    "model_1_results = evaluate_preds(y_true=tf.squeeze(test_labels),\n",
    "                                 y_pred=model_1_preds)\n",
    "model_1_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a93622",
   "metadata": {},
   "source": [
    "## Model 2 : Conv1D (WINDOW=30, HORIZON=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b07f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# model_2\n",
    "model_2 = Sequential(name='model_2_conv1d')\n",
    "model_2.add(layers.Lambda(lambda x : tf.expand_dims(x,axis=1)) )\n",
    "model_2.add(Conv1D(128,kernel_size=3,padding='same',activation='relu'))\n",
    "model_2.add(layers.Dense(HORIZON))\n",
    "\n",
    "# compile\n",
    "model_2.compile(loss='mae',\n",
    "               optimizer='adam',\n",
    "               metrics=['mae'])\n",
    "\n",
    "# fit\n",
    "model_2.fit(train_windows,\n",
    "           train_labels,\n",
    "           epochs=100,\n",
    "           validation_data=(test_windows,test_labels),\n",
    "           callbacks=[create_model_checkpoint(model_name=model_2.name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b442f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on test data\n",
    "model_2.evaluate(test_windows,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee6cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction\n",
    "model_2_preds = make_preds(model_2,test_windows)\n",
    "model_2_preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd6c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate metrics\n",
    "model_2_results = evaluate_preds(y_true=test_labels,\n",
    "                                y_pred=model_2_preds)\n",
    "model_2_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb289446",
   "metadata": {},
   "source": [
    "## Model 3 : RNN(WINDOW=30,HORIZON=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e115ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Let's build an LSTM model with the Functional API\n",
    "inputs = layers.Input(shape=(WINDOW_SIZE))\n",
    "x = layers.Lambda(lambda x: tf.expand_dims(x, axis=1))(inputs)\n",
    "x = layers.LSTM(128, activation=\"relu\")(x) \n",
    "output = layers.Dense(HORIZON)(x)\n",
    "model_3 = tf.keras.Model(inputs=inputs, outputs=output, name=\"model_3_lstm\")\n",
    "\n",
    "# Compile model\n",
    "model_3.compile(loss=\"mae\",\n",
    "                optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# Seems when saving the model several warnings are appearing: https://github.com/tensorflow/tensorflow/issues/47554 \n",
    "model_3.fit(train_windows,\n",
    "            train_labels,\n",
    "            epochs=100,\n",
    "            verbose=0,\n",
    "            batch_size=128,\n",
    "            validation_data=(test_windows, test_labels),\n",
    "            callbacks=[create_model_checkpoint(model_name=model_3.name)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc2ea65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_decision_forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60faf606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
